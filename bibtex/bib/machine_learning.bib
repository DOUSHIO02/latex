
# Commonly cited for the Harmonic F-measure
@book{vanrijsbergen1979ir,
      title     = {{Information Retrieval}},
      author    = {Van Rijsbergen, CJ},
      year      = {1979},
      publisher ={Butterworth-Heinemann Newton, MA, USA}
}

# On clustering
@phdthesis{sahami1998uml,
    title            = {{Using machine learning to improve information access}},
    author           = {Sahami, M.},
    year             = {1998},
    school           = {Stanford University}
}


# Porter stemmer
@article{porter1997ass,
    title            = {{An algorithm for suffix stripping (reprint)}},
    author           = {Porter, MF},
    journal          = {Readings in Information Retrieval},
    year             = {1997}
}

#LSI
@article{deerwester1990ils,
      title    = {{Indexing by latent semantic analysis}},
      author   = {Deerwester, S. and Dumais, S.T. and Furnas, G.W. and Landauer, T.K. and Harshman, R.},
      journal  = {Journal of the American Society for Information Science},
      volume   = {41},
      number   = {6},
      pages    = {391--407},
      year     = {1990},
      abstract = {A new method for automatic indexing and retrieval is
described. The approach is to take advantage of implicit higher-order structure
in the association of terms with documents (semantic structure) in order to
improve the detection of relevant documents on the basis of terms found in
queries. The particular technique used is singular-value decomposition, in
which a large term by document matrix is decomposed into a set of ca. 100
orthogonal factors from which the original matrix can be approximated by linear
combination. Documents are represented by ca. 100 item vectors of factor
weights. Queries are represented as pseudo-document vectors formed from
weighted combinations of terms, and documents with supra-threshold cosine
values are returned. Initial tests find this completely automatic method for
retrieval to be promising. (C) 1990 John Wiley & Sons, Inc.} 

}

# SVM for text
@article{joachims1998tcs,
      title     = {{Text categorization with support vector machines: Learning with many relevant features}},
      author    = {Joachims, T.},
      journal   = {Proceedings of ECML-98, 10th European Conference on Machine Learning},
      volume    = {1398},
      pages     = {137--142},
      year      = {1998},
      publisher ={Springer},
      abstract = {This paper explores the use of Support Vector Machines (SVMs)
for learning text classifiers from examples. It analyzes the particular
properties of learning with text data and identifies why SVMs are appropriate
for this task. Empirical results support the theoretical findings. SVMs achieve
substantial improvements over the currently best performing methods and behave
robustly over a variety of different learning tasks. Furthermore they are fully
automatic, eliminating the need for manual parameter tuning.}
}


# Vector space model
@article{salton1975vsm,
      title     = {{A vector space model for automatic indexing}},
      author    = {Salton, G. and Wong, A. and Yang, CS},
      journal   = {Communications of the ACM},
      volume    = {18},
      number    = {11},
      pages     = {613--620},
      year      = {1975},
      publisher = {ACM Press New York, NY, USA},
      abstract  = { In a document retrieval, or other pattern matching
environment where stored entities (documents) are compared with each other or
with incoming patterns (search requests), it appears that the best indexing
(property) space is one where each entity lies as far away from the others as
possible; in these circumstances the value of an indexing system may be
expressible as a function of the density of the object space; in particular,
retrieval performance may correlate inversely with space density. An approach
based on space density computations is used to choose an optimum indexing
vocabulary for a collection of documents. Typical evaluation results are shown,
demonstating the usefulness of the model.  }
}

# IDF
# NO SOURCE AVAILABLE
@article{jones1972sit,
      title     = {{A statistical interpretation of term specificity and its application in retrieval}},
      author    = {Jones, K.S. and others},
      journal   = {Journal of Documentation},
      volume    = {28},
      number    = {1},
      pages     = {11--21},
      year      = {1972},
      publisher ={Emerald Group Publishing Limited},
      abstract  = {
          The exhaustivity of document descriptions and the specificity of
index terms are usually regarded as independent. It is suggested that
specificity should be interpreted statistically, as a function of term use
rather than of term meaning. The effects on retrieval of variations in term
specificity are examined, experiments with three test collections showing, in
particular, that frequently-occurring terms are required for good overall
performance. It is argued that terms should be weighted according to collection
frequency, so that matches on less frequent, more specific, terms are of
greater value than matches on frequent terms. Results for the test collections
show that considerable improvements in performance are obtained with this very
simple procedure. }
}


@article{benjamini1995cfd,
  title={{Controlling the false discovery rate: a practical and powerful approach to multiple testing}},
  author={Benjamini, Y. and Hochberg, Y.},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={289--300},
  year={1995},
  publisher={Blackwell Publishers}
}

@article{verhoeven2005implementing,
      title     = {{Implementing false discovery rate control: increasing your power}},
      author    = {Verhoeven, K.J.F. and Simonsen, K.L. and McIntyre, L.M.},
      journal   = {Oikos},
      volume    = {108},
      number    = {3},
      pages     = {643--647},
      year      = {2005},
      publisher ={Citeseer}
}



@article{jones2004statistical,
      title     = {{A statistical interpretation of term specificity and its application in retrieval}},
      author    = {Jones, K.S. and others},
      journal   = {Journal of documentation},
      volume    = {60},
      pages     = {493--502},
      year      = {2004},
      publisher ={Emerald Group Publishing Limited}
}

@article{zaki2005efficient,
      title     = {{Efficient algorithms for mining closed itemsets and their lattice structure}},
      author    = {Zaki, M.J. and Hsiao, C.J.},
      journal   = {IEEE Transactions on Knowledge and Data Engineering},
      pages     = {462--478},
      year      = {2005},
      publisher ={IEEE Computer Society}
}



@conference{corduneanu2001variational,
  title={{Variational Bayesian model selection for mixture distributions}},
  author={Corduneanu, A. and Bishop, C.M.},
  booktitle={Proceedings Eighth International Conference on Artificial Intelligence and Statistics},
  pages={27--34},
  year={2001},
  organization={Citeseer}
}

@conference{somervuo2002speech,
  title={{Speech Modeling Using Variational Bayesian Mixture of Gaussians}},
  author={Somervuo, P.},
  booktitle={Seventh International Conference on Spoken Language Processing},
  year={2002},
  organization={ISCA}
}

@article{attias2000variational,
  title={{A variational Bayesian framework for graphical models}},
  author={Attias, H.},
  journal={Advances in neural information processing systems},
  volume={12},
  number={1-2},
  pages={209--215},
  year={2000}
}

@article{tibshirani1996regression,
      title     = {{Regression shrinkage and selection via the lasso}},
      author    = {Tibshirani, R.},
      journal   = {Journal of the Royal Statistical Society. Series B (Methodological)},
      volume    = {58},
      number    = {1},
      pages     = {267--288},
      year      = {1996},
      publisher ={Blackwell Publishers}
}

